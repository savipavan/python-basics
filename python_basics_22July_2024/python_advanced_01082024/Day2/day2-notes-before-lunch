**** Python Advanced workshop ****

2-Aug-2024 (10am to 6pm)


    ** Instructor name:
        Nirmalya Das, mail: ndas1971@gmail.com

    **Recommended Book:
           Core Python:Learning Python, Mark Lutz
           Adv Python: Programming python, Mark Lutz


    ** How to get data for workshop - Store them in your local directory
           iris.csv
           example.xml
           example.json
           example1.xml

    ** If you don't have anaconda, install below in command prompt or in devshell
            pip install poetry pytest pytest-cov requests  beautifulsoup4 flask sqlalchemy numpy  pandas  openpyxl xlrd matplotlib wheel gevent


    ** Daily Quiz:
    please Use same browser and same user name for all days

    Day1 Quiz:  https://bit.ly/4bTDwJR

    DAY2 Quiz: https://bit.ly/4bRnmR8

    **  Check your quiz result at:
    (Use the same browser where quiz is submitted)
    https://bit.ly/3XkqWPj


############################################################################################
############compre_ex.py
#Map pattern - process each element
lst = [2,4,7,9]

#Square of each number
#list - insertion ordered, duplicates
o = []
for e in lst:
    o.append( e*e )

print(o)
#equiv - list compre
o = [ e*e for e in lst ]
print(o)
#square only even nos
o = []
for e in lst:
    if e%2 == 0:
        o.append( e*e )
#equiv
o = [ e*e for e in lst if e%2 == 0]
print(o)

#Create a pairs of even and odd number
o = []
for e in lst:
    if e%2 == 0:
        for e1 in lst:
            if e1 % 2 == 1:
                o.append( ( e,e1) )
#equiv
o = [ ( e,e1) for e in lst if e%2 == 0
                for e1 in lst if e1 % 2 == 1]
print(o)


#set compre - set - unique - can have filter, multiple for etc
o = set()
for e in lst:
    o.add(e*e)
#equiv
o = { e*e for e in lst  }
print(o)
#dict compre - dict - k:v pairs , can have filters, multiple fors etc
o = {}
for e in lst:
    o[e] = e*e
#equiv
o = { e : e*e for e in lst  }
print(o)


##################XML processing
>>> path = r"D:\handson\DAY2\data\example.xml"
>>> import xml.etree.ElementTree as ET
>>> tr = ET.parse(path)
>>>
>>> root = tr.getroot()
>>> type(root)
<class 'xml.etree.ElementTree.Element'>
>>> dir(root)
['__class__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'attrib', 'clear', 'extend', 'find', 'findall', 'findtext', 'get', 'getchildren', 'getiterator', 'insert', 'items', 'iter', 'iterfind', 'itertext', 'keys', 'makeelement', 'remove', 'set', 'tag', 'tail', 'text']
>>> root.tag, root.attrib, root.text
('data', {}, '\n    ')
>>> #traversing
>>> #ex- get all ranks
>>> nn = root.findall("./country/rank")  # XPATH
>>> [n.text for n in nn]
['1', '4', '68']
>>> #XML -disadv - no DT, only string
>>> [int(n.text) for n in nn]
[1, 4, 68]
>>> #https://www.w3schools.com/xml/xpath_syntax.asp
>>> # // - search at any level, . - current ele
>>> # .. parent ...
>>> xn = root.findall(".//year/..[@name='Singapore']")
>>> xn
[<Element 'country' at 0x000001A29E692868>]
>>> root.findall(".//year")
[<Element 'year' at 0x000001A29E692728>, <Element 'year' at 0x000001A29E692908>, <Element 'year' at 0x000001A29E692A98>]
>>> root.findall(".//year/..")
[<Element 'country' at 0x000001A29E692688>, <Element 'country' at 0x000001A29E692868>, <Element 'country' at 0x000001A29E6929F8>]
>>> xn = root.findall(".//year/..[@name='Singapore']")
>>> xn
[<Element 'country' at 0x000001A29E692868>]
>>> [ n for n in root.findall(".//year/..") if n.attrib['name'] == 'Singapore']
[<Element 'country' at 0x000001A29E692868>]

>>> #Two disadv - not DT, only string, need to learn XPATH
>>> path = r"D:\handson\DAY2\data\example.json"
>>> import json
>>> with open(path) as f:
...     obj = json.load(f)
...
>>> obj
>>> #load/dump - file load/save
>>> #loads/dumps - string load/save
>>> type(obj)
<class 'list'>
>>> type(obj[0])
<class 'dict'>
>>> obj[0].keys()
dict_keys(['empId', 'details'])
>>> #get all emp id
>>> [emp['empId'] for emp in obj]
[1, 20]
>>> #Handson - get full names of all emp
>>>
>>> #HomeWork - 1. Get all the cities of all emp who are alive
>>> #2. get office phones of all alive emps
>>>
>>> #creation of json
>>> o = {'name': 'ok', 'age': 20}
>>> json.dumps(o)
'{"name": "ok", "age": 20}'
>>>>>> # install all external modules
>>> #list in online notepad
>>> #open pandas ref pdf - https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf
>>> import pandas as pd
>>> len(dir(pd))
142
>>> len(dir(pd.DataFrame))
432
>>> len(dir(pd.Series))
421
>>> #very good doc,  cheat sheet-good
>>> #OPEN a file
>>> path = r"D:\handson\DAY2\data\iris.csv"
>>> iris = pd.read_csv(path)
>>> type(iris)
<class 'pandas.core.frame.DataFrame'>
>>> #check metadata
>>> #rows x cols
>>> len(iris)
150
>>> iris.columns
Index(['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Name'], dtype='object')
>>> #datatypes of each cols
>>> iris.dtypes
SepalLength    float64
SepalWidth     float64
PetalLength    float64
PetalWidth     float64
Name            object
dtype: object
>>> #row_id is called index
>>> iris.index
RangeIndex(start=0, stop=150, step=1)
>>>>>> iris.head()
   SepalLength  SepalWidth  PetalLength  PetalWidth         Name
0          5.1         3.5          1.4         0.2  Iris-setosa
1          4.9         3.0          1.4         0.2  Iris-setosa
2          4.7         3.2          1.3         0.2  Iris-setosa
3          4.6         3.1          1.5         0.2  Iris-setosa
4          5.0         3.6          1.4         0.2  Iris-setosa
>>> #columns are VIP
>>> #columns - features, rows - example of those features
>>> #access
>>> iris.SepalLength  # object attr access
0      5.1
1      4.9
2      4.7
3      4.6
4      5.0
      ...
145    6.7
146    6.3
147    6.5
148    6.2
149    5.9
Name: SepalLength, Length: 150, dtype: float64
>>> type(iris['SepalLength'])
<class 'pandas.core.series.Series'>
>>> type(iris[['SepalLength', 'SepalWidth']])
<class 'pandas.core.frame.DataFrame'>
>>> # rows - iloc- based on index, loc-based row_id, col name
>>> iris.head()
   SepalLength  SepalWidth  PetalLength  PetalWidth         Name
0          5.1         3.5          1.4         0.2  Iris-setosa
1          4.9         3.0          1.4         0.2  Iris-setosa
2          4.7         3.2          1.3         0.2  Iris-setosa
3          4.6         3.1          1.5         0.2  Iris-setosa
4          5.0         3.6          1.4         0.2  Iris-setosa
>>> iris.loc[0:2, ['SepalLength' , 'SepalWidth']]
   SepalLength  SepalWidth
0          5.1         3.5
1          4.9         3.0
2          4.7         3.2
>>> iris.loc[0:2, :]
   SepalLength  SepalWidth  PetalLength  PetalWidth         Name
0          5.1         3.5          1.4         0.2  Iris-setosa
1          4.9         3.0          1.4         0.2  Iris-setosa
2          4.7         3.2          1.3         0.2  Iris-setosa
>>> #loc - start:end row_id, end row_id is included
>>> #.iloc - start:end index, end index is not included
>>> iris.iloc[0:3, [0,1]]
   SepalLength  SepalWidth
0          5.1         3.5
1          4.9         3.0
2          4.7         3.2
>>> #Creation of columns
>>> iris.head()
   SepalLength  SepalWidth  PetalLength  PetalWidth         Name
0          5.1         3.5          1.4         0.2  Iris-setosa
1          4.9         3.0          1.4         0.2  Iris-setosa
2          4.7         3.2          1.3         0.2  Iris-setosa
3          4.6         3.1          1.5         0.2  Iris-setosa
4          5.0         3.6          1.4         0.2  Iris-setosa
>>> iris['dummy'] = iris.SepalLength + 2* iris.PetalLength - 2
>>> iris.dummy
0       5.9
1       5.7
2       5.3
3       5.6
4       5.8
       ...
145    15.1
146    14.3
147    14.9
148    15.0
149    14.1
Name: dummy, Length: 150, dtype: float64
>>> #Aggregation
>>> iris.iloc[:, 0:4].describe()  # DF function
       SepalLength  SepalWidth  PetalLength  PetalWidth
count   150.000000  150.000000   150.000000  150.000000
mean      5.843333    3.054000     3.758667    1.198667
std       0.828066    0.433594     1.764420    0.763161
min       4.300000    2.000000     1.000000    0.100000
25%       5.100000    2.800000     1.600000    0.300000
50%       5.800000    3.000000     4.350000    1.300000
75%       6.400000    3.300000     5.100000    1.800000
max       7.900000    4.400000     6.900000    2.500000
>>> iris.Name.unique()
array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)
>>> gr = iris.groupby('Name')
>>> gr.mean()
                 SepalLength  SepalWidth  PetalLength  PetalWidth   dummy
Name
Iris-setosa            5.006       3.418        1.464       0.244   5.934
Iris-versicolor        5.936       2.770        4.260       1.326  12.456
Iris-virginica         6.588       2.974        5.552       2.026  15.692
>>> type(gr)
<class 'pandas.core.groupby.generic.DataFrameGroupBy'>
>>> dir(gr)
['Name', 'PetalLength', 'PetalWidth', 'SepalLength', 'SepalWidth', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_accessors', '_agg_examples_doc', '_agg_general', '_agg_py_fallback', '_aggregate_frame', '_aggregate_item_by_item', '_aggregate_with_numba', '_apply_allowlist', '_apply_filter', '_apply_to_column_groupbys', '_bool_agg', '_cache', '_can_use_transform_fast', '_choose_path', '_concat_objects', '_constructor', '_cumcount_array', '_cython_agg_general', '_cython_transform', '_define_paths', '_dir_additions', '_dir_deletions', '_fill', '_get_cythonized_result', '_get_data_to_aggregate', '_get_index', '_get_indices', '_gotitem', '_group_selection', '_hidden_attrs', '_insert_inaxis_grouper_inplace', '_internal_names', '_internal_names_set', '_iterate_column_groupbys', '_iterate_slices', '_make_wrapper', '_numba_prep', '_obj_1d_constructor', '_obj_with_exclusions', '_python_agg_general', '_python_apply_general', '_reindex_output', '_reset_cache', '_reset_group_selection', '_resolve_numeric_only', '_selected_obj', '_selection', '_selection_list', '_set_group_selection', '_set_result_index_ordered', '_transform', '_transform_general', '_transform_item_by_item', '_transform_with_numba', '_wrap_agged_manager', '_wrap_aggregated_output', '_wrap_applied_output', '_wrap_applied_output_series', '_wrap_transform_fast_result', '_wrap_transformed_output', 'agg', 'aggregate', 'all', 'any', 'apply', 'backfill', 'bfill', 'boxplot', 'corr', 'corrwith', 'count', 'cov', 'cumcount', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'dtypes', 'dummy', 'ewm', 'expanding', 'ffill', 'fillna', 'filter', 'first', 'get_group', 'groups', 'head', 'hist', 'idxmax', 'idxmin', 'indices', 'last', 'mad', 'max', 'mean', 'median', 'min', 'ndim', 'ngroup', 'ngroups', 'nth', 'nunique', 'ohlc', 'pad', 'pct_change', 'pipe', 'plot', 'prod', 'quantile', 'rank', 'resample', 'rolling', 'sample', 'sem', 'shift', 'size', 'skew', 'std', 'sum', 'tail', 'take', 'transform', 'tshift', 'var']
>>> gr.agg({'SepalLength': ['min', 'max', 'mean', 'count']})
                SepalLength
                        min  max   mean count
Name
Iris-setosa             4.3  5.8  5.006    50
Iris-versicolor         4.9  7.0  5.936    50
Iris-virginica          4.9  7.9  6.588    50
>>> gr.agg({'SepalLength': ['min', 'max', 'mean', 'count']}).to_excel("output.xlsx")
>>> help(pd.DataFrame.describe)
Help on function describe in module pandas.core.generic:

describe(self: 'FrameOrSeries', percentiles=None, include=None, exclude=None, datetime_is_numeric=False) -> 'FrameOrSeries'
    Generate descriptive statistics.

    Descriptive statistics include those that summarize the central
    tendency, dispersion and shape of a
    dataset's distribution, excluding ``NaN`` values.

    Analyzes both numeric and object series, as well
    as ``DataFrame`` column sets of mixed data types. The output
    will vary depending on what is provided. Refer to the notes
    below for more detail.

    Parameters
    ----------
    percentiles : list-like of numbers, optional
        The percentiles to include in the output. All should
        fall between 0 and 1. The default is
        ``[.25, .5, .75]``, which returns the 25th, 50th, and
        75th percentiles.
    include : 'all', list-like of dtypes or None (default), optional
        A white list of data types to include in the result. Ignored
        for ``Series``. Here are the options:

        - 'all' : All columns of the input will be included in the output.
        - A list-like of dtypes : Limits the results to the
          provided data types.
          To limit the result to numeric types submit
          ``numpy.number``. To limit it instead to object columns submit
          the ``numpy.object`` data type. Strings
          can also be used in the style of
          ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To
          select pandas categorical columns, use ``'category'``
        - None (default) : The result will include all numeric columns.
    exclude : list-like of dtypes or None (default), optional,
        A black list of data types to omit from the result. Ignored
        for ``Series``. Here are the options:

        - A list-like of dtypes : Excludes the provided data types
          from the result. To exclude numeric types submit

>>> type(iris.iloc[:, 0:4].describe())
<class 'pandas.core.frame.DataFrame'>
>>> iris.iloc[:, 0:4].describe().iloc[0:2,0:2]
       SepalLength  SepalWidth
count   150.000000     150.000
mean      5.843333       3.054
>>> #plot
>>> import matplotlib.pyplot as plt
>>> iris.iloc[:, 0:4].plot(kind='line')
<matplotlib.axes._subplots.AxesSubplot object at 0x000001A2A2173488>
>>> plt.savefig("p.png")
>>> #DBAPI - based on sql OR orm
>>> #sql stmt and execute - SQL DB API
>>> #orm - table is mapped into a class
>>> #oracle/mysql/sql.....
>>> #mysql - 7 libs/modules
>>> from sqlalchemy import create_engine
>>> #https://docs.sqlalchemy.org/en/14/core/engines.html
>>> #single host lazy DF - Dask
>>> #multihost cluster DF - pyspark
>>> engine = create_engine("sqlite:///foo.db")
>>> #table =iris
>>> iris.to_sql("iris", con=engine)
>>> sql = "select max(SepalLength) from iris group by Name"
>>> from sqlalchemy import text
>>> with engine.connect() as con:
...     print(con.execute(text(sql)).fetchall())
...
[(5.8,), (7.0,), (7.9,)]


# C:\Users\varri\Videos\Movavi Screen Recorder